{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [You can find all the data here](http://shakespeare.mit.edu/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from utils import SentenceReader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    def __init__(self, dir_name, input_size, w2v_vector_size):\n",
    "        self.dir_name = dir_name\n",
    "        self.w2v_vector_size = w2v_vector_size\n",
    "        self.input_size = input_size\n",
    "        self.w2v_model = None\n",
    "        self.x_train = None\n",
    "        self.y_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "\n",
    "    def get_word_index(self, word):\n",
    "        return self.w2v_model.wv.vocab[word].index\n",
    "\n",
    "    def get_word_from_one_hot_matrix(self, one_hot_matrix):\n",
    "        return [\n",
    "            self.w2v_model.wv.index2word[int(one_hot_vector.argmax())]\n",
    "            for one_hot_vector in one_hot_matrix\n",
    "        ]\n",
    "        \n",
    "    def get_word_index_to_vec(self, word_index):\n",
    "        word = self.w2v_model.wv.index2word[word_index]\n",
    "        return self.w2v_model[word]\n",
    "\n",
    "    def word_to_vec(self, word):\n",
    "        return self.w2v_model[word]\n",
    "\n",
    "    def get_x_y_from_word_sequence(self, sequence):\n",
    "        n = self.input_size+1\n",
    "        m = len(sequence) - n + 1\n",
    "        seq_matrix = np.zeros([m, n], dtype=np.float32)\n",
    "\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                try:\n",
    "                    seq_matrix[i, j] = sequence[i+j]\n",
    "                except Exception as e:\n",
    "                    print(i, j, i+j)\n",
    "                    raise e\n",
    "\n",
    "        x = np.zeros([m, self.input_size, self.w2v_vector_size], dtype=np.float32)\n",
    "        for i in range(m):\n",
    "            for j in range(self.input_size):\n",
    "                word_index = int(seq_matrix[i][j])\n",
    "                x[i, j] = self.get_word_index_to_vec(word_index)\n",
    "\n",
    "        y = np.zeros([m, self.w2v_vector_size], np.float32)\n",
    "        for i in range(m):\n",
    "            word_index = int(seq_matrix[:, -1][i])\n",
    "            y[i, :] = self.get_word_index_to_vec(word_index)\n",
    "        # corpus_count = len(r.w2v_model.wv.index2word)\n",
    "        # y = np.zeros([m, corpus_count], np.int8)\n",
    "        # y_mask = seq_matrix[:, -1].reshape(-1, ).astype(np.int8)\n",
    "        # y[np.arange(m), y_mask] = 1\n",
    "        return x, y\n",
    "\n",
    "    def get_unit_matrix(matrix):\n",
    "        mod = np.power((matrix**2).sum(1), 0.5).reshape(matrix.shape[0], 1)\n",
    "        return matrix / mod\n",
    "\n",
    "    def read_data(self, test_size=0.2):\n",
    "        sentences = list(SentenceReader(self.dir_name))\n",
    "        self.w2v_model = Word2Vec(sentences, size=self.w2v_vector_size, min_count=1)\n",
    "\n",
    "        word_index_sequence = []\n",
    "        for sentence in sentences:\n",
    "            word_index_sequence.extend([self.get_word_index(word) for word in sentence])\n",
    "        x, y = self.get_x_y_from_word_sequence(word_index_sequence)\n",
    "\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(\n",
    "            x, y, test_size=test_size, random_state=1211\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_input = 3\n",
    "n_vectors = 60\n",
    "\n",
    "n_hidden = 512\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file .ipynb_checkpoints\n",
      "reading file romeo_and_juliet_1000.txt\n"
     ]
    }
   ],
   "source": [
    "r = DataReader('data/shakespear/small/', n_input, n_vectors)\n",
    "r.read_data()\n",
    "vocab_size = len(r.w2v_model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, n_input, n_vectors), name='x')\n",
    "y = tf.placeholder(tf.float32, (None, n_vectors), name='y')\n",
    "\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_vectors]), name='weights_out')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_vectors]), name='biases_out')\n",
    "}\n",
    "\n",
    "def RNN(x, w, b):\n",
    "    x = tf.unstack(x, n_input, 1)\n",
    "    # rnn_cell = tf.contrib.rnn.MultiRNNCell([rnn.GRUCell(n_hidden) for _ in range(3)])\n",
    "    rnn_cell = rnn.BasicLSTMCell(n_hidden)\n",
    "    output, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "    return tf.matmul(output[-1], w['out']) + b['out']\n",
    "\n",
    "# def cosine_loss(pred, labels):\n",
    "#     pred_mod = tf.pow(tf.pow(pred, 2).sum(1), 0.5).reshape(matrix.shape[0], 1)\n",
    "#     return matrix / mod\n",
    "\n",
    "model = RNN(x, weights, biases)\n",
    "cost = tf.reduce_mean(tf.squared_difference(model, y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "def train(session, batch_size, epochs):\n",
    "    m = r.x_train.shape[0]\n",
    "    for e in range(epochs):\n",
    "        for i in range(0, m, batch_size):\n",
    "            x_batch = r.x_train[i:i+batch_size, :]\n",
    "            y_batch = r.y_train[i:i+batch_size, :]\n",
    "            session.run(optimizer, feed_dict={x: x_batch, y: y_batch})\n",
    "        c = session.run(cost, feed_dict={x: r.x_test[:2000], y: r.y_test[:2000]})\n",
    "        print('{}. Cost - {}'.format(e, c))\n",
    "\n",
    "def predict(session, x_matrix):\n",
    "    return session.run(model, feed_dict={x: x_matrix})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Cost - 0.5544190406799316\n",
      "1. Cost - 0.35279542207717896\n",
      "2. Cost - 0.33756324648857117\n",
      "3. Cost - 0.3308546543121338\n",
      "4. Cost - 0.32579174637794495\n",
      "5. Cost - 0.3224600553512573\n",
      "6. Cost - 0.3214806914329529\n",
      "7. Cost - 0.3173556923866272\n",
      "8. Cost - 0.3148678243160248\n",
      "9. Cost - 0.31495580077171326\n",
      "10. Cost - 0.31402137875556946\n",
      "11. Cost - 0.3123234808444977\n",
      "12. Cost - 0.31293049454689026\n",
      "13. Cost - 0.3115520477294922\n",
      "14. Cost - 0.3104085326194763\n",
      "15. Cost - 0.3104018270969391\n",
      "16. Cost - 0.30949828028678894\n",
      "17. Cost - 0.3093410134315491\n",
      "18. Cost - 0.30849018692970276\n",
      "19. Cost - 0.3082958161830902\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "session.run(init)\n",
    "train(session, 2048, 20)\n",
    "predictions = predict(session, r.x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = predict(session, r.x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def represent_x_and_y(x, y):\n",
    "    for i in range(x.shape[0]):\n",
    "        input_sequence = []\n",
    "        for j in range(x.shape[1]):\n",
    "            word, _ = r.w2v_model.wv.similar_by_vector(x[i][j], topn=1)[0]\n",
    "            input_sequence.append(word)\n",
    "        output_word, _ = r.w2v_model.similar_by_vector(y[i], topn=1)[0]\n",
    "        print('{} => {}'.format(' '.join(input_sequence), output_word))\n",
    "\n",
    "def generate_sentence(session, length, start_words):\n",
    "    n_start_words = len(start_words)\n",
    "    sequence_words = []\n",
    "    sequence_matrix = np.zeros([length, r.w2v_vector_size], dtype=np.float32)\n",
    "    for i, word in enumerate(start_words):\n",
    "        sequence_matrix[i, :] = r.word_to_vec(word)\n",
    "        sequence_words.append(word)\n",
    "\n",
    "    for i in range(length-n_start_words):\n",
    "        input_sequence = sequence_matrix[i: i+r.input_size, :].reshape([1, r.input_size,-1])\n",
    "        # print(input_sequence.shape, input_sequence.sum())\n",
    "        output_vector = predict(session, input_sequence)\n",
    "        sequence_matrix[i+n_start_words, :] = output_vector\n",
    "\n",
    "        word, _ = r.w2v_model.wv.similar_by_vector(output_vector.reshape([-1, ]), topn=1)[0]\n",
    "        sequence_words.append(word)\n",
    "    return ' '.join(sequence_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. Why is this place so whether quite quite quite quite quite best best best best just just just just just just just just just going going going going going going going going going shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn shouldn'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(session, 100, '. Why is this place so'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(322084, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.random_uniform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn.BasicLSTMCell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_cell = rnn.BasicLSTMCell(n_hidden)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
